# 金融序列预测

![1741601795706](images/简历内容/1741601795706.png)

**挑战与任务**：时间序列与自然语言对齐
**该方法与传统训练方法比较：**
![1741604389257](images/简历内容/1741604389257.png)
**模型介绍：**

1. instance Norm
2. patching ：获取每一小部分的时间序列窗口，移动窗口，聚合局部特征
3. patch embedder：批处理向量化
4. text prototypes：先将预训练好的embedding与linear连接在一起，得到形状更小的embedding(prototypes翻译为原型)
5. multi-head attention：patch-embedding的输出作为q查询，prototypes为k，v，全连接层让输出改为合适的形状
6. pap（prompt-as-prefix）：提示词放到数据的前边

# YOLO算法：

### 指标分析：

map：

![1741695783231](images/简历内容/1741695783231.png)

IOU：交并比

# 专业技能：

### word2vec：

![1741612514637](images/简历内容/1741612514637.png)

**CBOW连续词袋模型（用于预测词语n）**：

![1741611794680](images/简历内容/1741611794680.png)

n+2，n+1，n-1，n-2的独热码与embedding层相乘，得到形状为4*128的向量为特征向量取平均得到1*128的n的特征向量，通过线性层计算的到输出向量，再经过softmax函数得到目标词（n）的独热码，作为预测结果

以下为实现代码（这里没有设置softmax）：

![1741612294159](images/简历内容/1741612294159.png)

**跳字模型（skip-gram）：**

skip模型在迭代的过程中，目标词的词向量会向上下文的词向量靠近
衡量两个词向量是否相似：使用词向量的点积：

$$
\mathbf{a} \cdot \mathbf{b}=\sum_{i=1}^{n} a_{i} b_{i}
$$

理解：$\mathbf{a} \cdot \mathbf{b}=|A| \cdot|B|\cdot\cos\alpha$，$\cos\alpha$最大，就最相近，为1时就是$A \cdot B $

![1741613383646](images/简历内容/1741613383646.png)

**GloVe：**

思想：

一个是基于奇异值分解SVD的lSA算法。
另一个方法是word2vec算法，该算法可以分为skip-gram和 continuous bag-of-words（CBOW）两类。
一个是利用了全局特征的矩阵分解方法，一个是利用局部上下文的方法。GloVe模型就是将这两中特征合并到一起的。

构建词语共现矩阵：

![1741664850396](images/简历内容/1741664850396.png)

真正的共现矩阵是：共现次数和权重递减函数（距离越小，权重越大）的乘积

![1741665154912](images/简历内容/1741665154912.png)
![1741665453971](images/简历内容/1741665453971.png)

**字词嵌入：** fastText

原理：

- 子词建模：将每个词分解为多个子词（n-grams），这些子词可以是字符级别的子串
- 词向量的学习方式：Skip-gram模型（通过给定上下文词来预测目标词）和CBOW模型（通过给定目标词来预测上下文词）

优势：

1. **处理稀有词和未见过的词**：由于通过子词进行建模，FastText可以有效地处理未在训练数据中出现的词。这对于许多现实场景中存在大量稀有或新词的应用（例如社交媒体、新闻等）特别有用。
2. **考虑词的形态学特征**：FastText可以通过分解词的子词来捕捉词的形态学特征，比如词根、词尾、前缀等。这使得它在处理具有形态变化的语言（如德语、法语等）时特别有效。
3. **训练速度快**：尽管FastText比Word2Vec更加复杂，但它在训练上通常是比较高效的，尤其是在大规模语料库上。例如，在标准的多核CPU上，FastText能够在10分钟之内训练10亿词级别语料库的词向量，能够在一分钟之内分类有着30万多类别的50多万句子。
4. **高效的稀疏数据处理**：FastText在处理低频词或稀有词时比传统的词向量模型表现更好，尤其是在面对词汇量较大的情况时。
5. **多语言支持**：FastText能够很好地处理不同语言的文本，无需进行复杂的预处理，特别适合多语言环境下的文本分类任务。
6. **易于部署**：FastText模型简单，参数少，易于在各种硬件上部署，包括资源受限的环境。
7. **可扩展性**：FastText支持在线学习，可以轻松地更新模型以适应新的数据。

缺点：

1. **忽视词汇顺序与语法结构**：虽然n-gram引入了一定的顺序信息，但FastText依然无法完全捕捉复杂的句法和语义依赖。这是因为其隐层是通过简单的求和取平均得到的，丢失了词顺序的信息。
2. **对罕见词敏感**：词袋模型可能导致罕见词在特征向量中占主导地位，影响分类性能。
3. **需要预训练词嵌入**：若使用词嵌入，需预先训练或获取高质量的词向量，增加了模型准备的复杂性。
4. **对大规模数据的处理能力有限**。
5. **对于某些特定领域的任务可能需要大量的标注数据**。
6. **对于某些复杂的任务可能需要调整模型结构或参数设置**。
7. **容易过拟合**：在分类类别比较小或者数据集比较少的情况下，FastText容易过拟合。

字节对编码算法：

1. 首先，我们将符号词表初始化为所有英文小写字符、特殊的词尾符号`'_'`和特殊的未知符号`'[UNK]'`。
2. 统计词频
3. 开始迭代到指定次数：
   1. 返回词内最频繁的连续符号对，其中词来自输入词典`token_freqs`的键。
   2. 合并最频繁的连续符号对以产生新符号并加入到词典`token_freq`

直觉感受：字词嵌入是更为细粒度的学习语料库

# 知识点补充：

### 卷积形状的计算公式

### embedding

**作用：** 将文本转换为向量

**优势：**

1. 高维稀疏的输入数据（如单词、类别标签等）转换为低维稠密的向量表示，可以大幅降低数据存储和计算量
2. 低维稠密向量捕获了输入之间的语义和上下文信息，语义相近、类别相近的单词或者类别，其表示向量相似度也高，使得模型能够更好地理解数据信息并进行预测推理。

**实现原理：**
[embedding类型梳理](https://blog.csdn.net/qq_39172059/article/details/136661824)

### TF-IDF（term frequency–inverse document frequency，词频-逆向文件频率）：

**TF-IDF的主要思想是**：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

**1）TF是词频(Term Frequency)**

词频（TF）表示词条（关键字）在文本中出现的频率。这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件。

**2） IDF是逆向文件频率(Inverse Document Frequency)**

逆向文件频率 (IDF) ：某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。

如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。
**3）TF-IDF实际上是：TF \* IDF**

某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。

注：TF-IDF算法非常容易理解，并且很容易实现，但是其简单结构并没有考虑词语的语义信息，无法处理一词多义与一义多词的情况。

```python
def feature_select(list_words):
    #总词频统计
    doc_frequency=defaultdict(int)
    for word_list in list_words:
        for i in word_list:
            doc_frequency[i]+=1
 
    #计算每个词的TF值
    word_tf={}  #存储没个词的tf值
    for i in doc_frequency:
        word_tf[i]=doc_frequency[i]/sum(doc_frequency.values())
 
    #计算每个词的IDF值
    doc_num=len(list_words)
    word_idf={} #存储每个词的idf值
    word_doc=defaultdict(int) #存储包含该词的文档数
    for i in doc_frequency:
        for j in list_words:
            if i in j:
                word_doc[i]+=1
    for i in doc_frequency:
        word_idf[i]=math.log(doc_num/(word_doc[i]+1))
 
    #计算每个词的TF*IDF的值
    word_tf_idf={}
    for i in doc_frequency:
        word_tf_idf[i]=word_tf[i]*word_idf[i]
```

### 准确度(accuracy)、精确率（precision)、召回率（recall）、F1值

TP（True Positives)：真正例，预测为正例而且实际上也是正例；

FP（False Positives)：假正例，预测为正例然而实际上却是负例；

FN（false Negatives)：假负例，预测为负例然而实际上却是正例；

TN（True Negatives)：真负例，预测为负例而且实际上也是负例。

![1741694684288](images/简历内容/1741694684288.png)

准确率 Accuracy

准确度：正例和负例中预测正确数量占总数量的比例，用公式表示：

![1741694798479](images/简历内容/1741694798479.png)

精确度 Precision

精确度（查准率）：以预测结果为判断依据，预测为正例的样本中预测正确的比例。预测为正例的结果分两种，要么实际是正例TP，要么实际是负例FP，则可用公式表示:

![1741695159931](images/简历内容/1741695159931.png)

召回率 Recall

召回率（查全率）：以实际样本为判断依据，实际为正例的样本中，被预测正确的正例占总实际正例样本的比例。实际为正例的样本中，要么在预测中被预测正确TP，要么在预测中预测错误FN，用公式表示：

![1741694922288](images/简历内容/1741694922288.png)

F1值是精确度和召回率的调和平均值

![1741695312272](images/简历内容/1741695312272.png)

### RAG技术基本原理

**前期问题：** 大模型无法处理长文本问题
**更新大模型知识库设计：**

1. 微调模型：问题在于效果不稳定，少量文本容易被遗忘；成本高昂
2. 增强模型上下文长度：GLM4-long
3. 知识检索：提取文本一部分的内容，结合提示词，把知识库的相关文档片段“检索出来”

**检索技术发展迭代：**

1. 长文本切分成小文本，将用户问题短文档进行匹配，找到最相关的topk文档，发送给大模型引导用户回答相关问题
2. 微软的RAG graph

**RAG技术核心步骤与通用优化方法：**
![1741958286679](images/简历内容/1741958286679.png)

**技术难点：**每个步骤的技术优化

**RAG实践落地项目**：

- 手动搭建
- 使用langchain 、  Llama-index等开源项目快速搭建
- 使用GLM、openAI Assistant API等快速实现

