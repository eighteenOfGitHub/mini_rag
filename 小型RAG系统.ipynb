{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import PyPDF2\n",
    "import markdown\n",
    "import html2text\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display, Code, Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "api_key = 'your api'\n",
    "base_url = \"your url\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化客户端\n",
    "client = OpenAI(api_key=api_key, base_url = base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 临时设置环境变量\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "os.environ[\"OPENAI_BASE_URL\"] = base_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.RAG技术原理\n",
    "\n",
    "### 1.1 RAG技术必要性介绍\n",
    "\n",
    "LLM生成语言时，产生幻觉或不准确的结果\n",
    "\n",
    " - 信息误导：过时或不准确\n",
    " - 知识更新之后：无法访问新的技术\n",
    " - 推理能力不足：LLM在面对复杂推理任务时，难以提供高效且准确的结果\n",
    " \n",
    " ### 1.2 RAG系统的核心组件说明\n",
    "\n",
    " 1. **向量化模块：**用于将文档片段转化为向量表示，以便后续检索\n",
    " 2. **文档加载与切分模块：**负责加载文档并将其切分为若干易于处理的文档片段\n",
    " 3. **数据库模块：**用于存储文档片段及其对应的向量表示\n",
    " 4. **检索模块：**根据用户输入的查询，从数据库中检索最相关的文档片段\n",
    " 5. **生成模块：**将检索到的文档片段与用户输入的查询结合，生成最终的回答\n",
    "\n",
    " ### 1.3 RAG系统的基本流程：\n",
    "\n",
    " - **索引：**将文本库分割成焦段的Chunk，并通过编码器构建向量库索引\n",
    " - **检索：**根据问题和chunks的相似度检索相关文档片段\n",
    " - **生成：**一检索到的上下文为条件，生成问题的回答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OpenAI第三档embedding模型介绍与调用方法\n",
    "\n",
    "### 2.1 OpenAI Embedding模型介绍\n",
    "\n",
    "将文本字符串表示为向量（浮点数列表），通过计算向量之间的距离来衡量文本之间的相关性，向量距离越小，表示文本之间的相关性越高；反之越低。常见embedding应用包括：\n",
    "\n",
    " - 搜索：根据文本查询的相关性对结果进行排序\n",
    " - 聚类：根据文本相似性将其分组\n",
    " - 推荐：根据相关文本字符串推荐项目\n",
    " - 异常检测：识别与其他内容相关性较低的异常点\n",
    " - 多样性测量：分析相似性分布\n",
    " - 分类：将文本字符串根据其最相似的标签进行分类\n",
    "\n",
    "OpenAI最新的embedding模型是text-embedding-3-small和text-embedding-3-large向量长度分别为1536和3072的向量。用户可以设置维度参数来介绍向量的维度，不损失其表达概念的能力   \n",
    "\n",
    "### 2.2OpenAI Embedding模型获取方法（付费）：\n",
    "要获取文本的Embedding向量，可以将文本字符串发送到OpenAI的EmbeedingAPI断点，并指定所使用的模型\n",
    "\n",
    "这里用阿里云百炼embedding的模型替代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用embedding API 获取文本的向量表示\n",
    "response = client.embeddings.create(\n",
    "    input = \"测试文本\",\n",
    "    model = \"text-embedding-v3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "余弦相似度与效果介绍\n",
    "\n",
    "$$\n",
    "\\text { Cosine Similarity }(\\vec{a}, \\vec{b})=\\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\|\\|\\vec{b}\\|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consine_sim(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    magnitude = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    return dot_product / magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '我喜欢吃苹果'\n",
    "text2 = '苹果是我最喜欢吃的水果'\n",
    "text3 = '我喜欢用苹果手机'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = client.embeddings.create(\n",
    "    input = text1,\n",
    "    model = \"text-embedding-v3\"\n",
    ").data[0].embedding\n",
    "\n",
    "vector2 = client.embeddings.create(\n",
    "    input = text2,\n",
    "    model = \"text-embedding-v3\"\n",
    ").data[0].embedding\n",
    "\n",
    "vector3 = client.embeddings.create(\n",
    "    input = text3,\n",
    "    model = \"text-embedding-v3\"\n",
    ").data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consine_sim(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consine_sim(vector1, vector3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consine_sim(vector2, vector3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据实验，embedding模型可以根据句意相关的embedding的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbeddings:\n",
    "    \"\"\"\n",
    "    向量化的基类，用于将文本转化为向量表示，不同的子类可以实现不同的向量获取方式\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str, is_api: bool) -> None:\n",
    "        \"\"\"\n",
    "        初始化基类\n",
    "\n",
    "        参数：\n",
    "        path(str) - 如果是本地模型， path 表示模型路径；如果是api模式，则path可以为空\n",
    "        is_api(bool) - 是否使用api模式\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.is_api = is_api\n",
    "\n",
    "    def get_embedding(self, text:str, model: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        抽象方法：用于获取文本的向量表示\n",
    "\n",
    "        参数：\n",
    "\n",
    "        text(str) - 待获取向量表示的文本\n",
    "        model(str) - 模型名称\n",
    "\n",
    "        返回值：\n",
    "\n",
    "        List[float] - 文本的向量表示\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"子类必须实现该方法\")\n",
    "        \n",
    "    @classmethod\n",
    "    def cosine_similarity(cls, vector1: List[float], vector2: List[float]) -> float:\n",
    "        \"\"\"\n",
    "        计算两个向量之间的余弦相似度，用于衡量他们的相似程度\n",
    "\n",
    "        参数：\n",
    "        vector1（list[float]）- 第一个向量\n",
    "        vector2（list[float]）- 第二个向量\n",
    "        \n",
    "        返回：\n",
    "        float - 预选相似度值，范围从 -1 到 1，越接近 1 表示向量越相似\n",
    "        \"\"\"\n",
    "        dot_product = np.dot(vector1, vector2)\n",
    "        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)\n",
    "        if not magnitude:\n",
    "            return 0\n",
    "        return dot_product / magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIEmbedding(BaseEmbeddings):\n",
    "    \"\"\"\n",
    "    使用 OpenAI 的 Embedding API 来获取文本向量的类，继承自BaseEmbeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str = '', is_api: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        初始化类，设置 OpenAI API 客户端，如果使用的是 API 调用\n",
    "\n",
    "        参数：\n",
    "        path(str) - 如果是本地模型， path 表示模型路径；如果是api模式，则path可以为空\n",
    "        is_api(bool) - 是否使用api模式\n",
    "        \"\"\"\n",
    "        super().__init__(path, is_api)\n",
    "        if self.is_api:\n",
    "            # 初始化 OpenAI API 客户端\n",
    "            self.client = OpenAI()\n",
    "            self.client.api_key = os.getenv('OPENAI_API_KEY')\n",
    "            self.client.base_url = os.getenv('OPENAI_BASE_URL')\n",
    "\n",
    "    def get_embedding(self, text: str, model: str = 'text-embedding-v3') -> List[float]:\n",
    "        \"\"\"\n",
    "        使用 Open API 获取文本的向量表示\n",
    "\n",
    "        参数：\n",
    "        text(str) - 待获取向量表示的文本\n",
    "        model(str) - 模型名称\n",
    "\n",
    "        返回值：\n",
    "        List[float] - 文本的向量表示\n",
    "        \"\"\"\n",
    "        if self.is_api:\n",
    "            text = text.replace('\\n', ' ')\n",
    "            return self.client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "        else:\n",
    "            raise NotImplementedError(\"本地模型尚未实现\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbedding()\n",
    "text = \"这是一个实例文本，用于演示 OpenAI Embedding 的使用。\"\n",
    "embedding_vector = embedding_model.get_embedding(text)\n",
    "print(\"文本的向量表示为：\", embedding_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = embedding_model.get_embedding(text1)\n",
    "vector2 = embedding_model.get_embedding(text2)\n",
    "similarity = embedding_model.cosine_similarity(vector1, vector2)\n",
    "print(f\"两段文本的余弦相似度为：{similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.文档加载与切分模块创建\n",
    "\n",
    "### 3.1 文档格式处理函数\n",
    "\n",
    "展示支持多种格式的简单实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_content(cls, file_path: str):\n",
    "    # 根据文件扩展名选择读取方式\n",
    "    if file_path.endswith('.pdf'):\n",
    "        return cls.read_pdf(file_path)\n",
    "    elif file_path.endswith('.md'):\n",
    "        return cls.read_markdown(file_path)\n",
    "    elif file_path.endswith('.txt'):\n",
    "        return cls.read_text(file_path)\n",
    "    else:\n",
    "        raise ValueError('Unsupported file type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 文档切分函数\n",
    "\n",
    "按照token长度进行切分，设置一个最大Token长度，然后按照这个长度进行切分。在这个过程我们也会保证每个片段之间有一定的重叠，避免重要信息被切掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk(cls, text:str, max_token_len:int = 600, cover_content:int=150):\n",
    "    chunk_text = []\n",
    "    curr_len = 0\n",
    "    curr_chunk = ''\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.replace(' ', '')\n",
    "        line_len = len(enc.encode(line))\n",
    "        if line_len > max_token_len:\n",
    "            print('warning line_len = ', line_len)\n",
    "        if curr_len + line_len <= max_token_len:\n",
    "            curr_chunl += line + '\\n'\n",
    "            curr_len += line_len + 1\n",
    "        else:\n",
    "            chunk_text.append(curr_chunk)\n",
    "            curr_chunk = curr_chunk[-cover_content:] + line\n",
    "            curr_len = len(enc.encode(curr_chunk))\n",
    "\n",
    "    if curr_chunk:\n",
    "        chunk_text.append(curr_chunk)\n",
    "    \n",
    "    return chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要梯子\n",
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "len(enc.encode('你好啊,好久不见！'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadFiles:\n",
    "    \"\"\"\n",
    "    读取文件类，用于从指定路径读取支持的文件类型，并进行切分\n",
    "    当前支持类型：pdf, md, txt\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path:str) -> None:\n",
    "        \"\"\"\n",
    "        初始化函数，设定要读取的文件路径，并获取该路径下所有符合要求的文件。\n",
    "        ：param path：文件夹路径\n",
    "        \"\"\"\n",
    "        self._path = path\n",
    "        self.file_list = self.get_files() \n",
    "\n",
    "    def get_files(self):\n",
    "        \"\"\"\n",
    "        遍历指定文件夹，获取支持的文件类型列表（txt, md, pdf）\n",
    "        return 文件路径列表\n",
    "        \"\"\"\n",
    "        file_list = []\n",
    "        for filepath, dirnames, filenames in os.walk(self._path):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.md'):\n",
    "                    file_list.append(os.path.join(filepath, filename))\n",
    "                elif filename.endswith('.pdf'):\n",
    "                    file_list.append(os.path.join(filepath, filename))\n",
    "                elif filename.endswith('.txt'):\n",
    "                    file_list.append(os.path.join(filepath, filename))\n",
    "        return file_list\n",
    "    \n",
    "    def get_content(self, max_token_len:int = 600, cover_content: int = 150):\n",
    "        \"\"\"\n",
    "        读取文件内容并进行分割，将长文本切分为多个块\n",
    "        param max_token_len: 每个文档片段最大 Token 长度\n",
    "        prarm cover_content: 每个文档片段之间的 Token 长度\n",
    "        return： 切分后的文档片段列表\n",
    "        \"\"\"\n",
    "        docs = []\n",
    "        for file in self.file_list:\n",
    "            content = self.read_file_content(file)\n",
    "            chunk_content = self.get_chunk(content, max_token_len=max_token_len, cover_content=cover_content)\n",
    "            docs.extend(chunk_content)\n",
    "        return docs\n",
    "\n",
    "    @classmethod\n",
    "    def get_chunk(cls, text:str, max_token_len:int = 600, cover_content:int=150):\n",
    "        chunk_text = []\n",
    "        curr_len = 0\n",
    "        curr_chunk = ''\n",
    "        token_len = max_token_len - cover_content\n",
    "        lines = text.splitlines()\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.replace(' ', '')\n",
    "            line_len = len(enc.encode(line))\n",
    "            if line_len > max_token_len:\n",
    "                # 如果但凡长度超过限制，将其分给为多个片段\n",
    "                num_chunks = (line_len + token_len - 1) // token_len\n",
    "                for i in range(num_chunks):\n",
    "                    start = i * token_len\n",
    "                    end = start + token_len\n",
    "                    # 防止跨单词分割\n",
    "                    while not line[start:end].rstrip().isspace():\n",
    "                        start += 1\n",
    "                        end += 1\n",
    "                        if start >= line_len:\n",
    "                            break\n",
    "                    curr_chunk = curr_chunk[-cover_content:] + line[start:end]\n",
    "                    chunk_text.append(curr_chunk)\n",
    "                start = (num_chunks - 1) * token_len\n",
    "                curr_chunk = curr_chunk[-cover_content:] + line[start:end]\n",
    "                chunk_text.append(curr_chunk)\n",
    "            elif curr_len + line_len <= max_token_len:\n",
    "                # 当前片段长度未超过限制时，继续累加\n",
    "                curr_chunk += line + '\\n'\n",
    "                curr_len += line_len + 1\n",
    "            else:    \n",
    "                chunk_text.append(curr_chunk)\n",
    "                curr_chunk = curr_chunk[-cover_content:] + line\n",
    "                curr_len = len(enc.encode(curr_chunk))\n",
    "\n",
    "        if curr_chunk:\n",
    "            chunk_text.append(curr_chunk)\n",
    "        \n",
    "        return chunk_text\n",
    "    \n",
    "    @classmethod\n",
    "    def read_file_content(cls, file_path:str):\n",
    "        \"\"\"\n",
    "        读取文件内容，根据文件类型选择不同的读取方式\n",
    "        param file_path: 文件路径\n",
    "        return： 文件内容\n",
    "        \"\"\"\n",
    "\n",
    "        if file_path.endswith('.pdf'):\n",
    "            return cls.read_pdf(file_path)\n",
    "        elif file_path.endswith('.md'):\n",
    "            return cls.read_markdown(file_path)\n",
    "        elif file_path.endswith('.txt'):\n",
    "            return cls.read_text(file_path)\n",
    "        else:\n",
    "            raise ValueError('Unsupported file type')\n",
    "        \n",
    "    @classmethod\n",
    "    def read_pdf(cls, file_path:str):\n",
    "        \"\"\"\n",
    "        读取pdf文件内容\n",
    "        param file_path: 文件路径\n",
    "        return： 文件内容\n",
    "        \"\"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            text = ''\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                text += reader.pages[page_num].extract_text()\n",
    "            return text\n",
    "\n",
    "    @classmethod\n",
    "    def read_markdown(cls, file_path:str):\n",
    "        \"\"\"\n",
    "        读取markdown文件内容\n",
    "        param file_path: 文件路径\n",
    "        return： 文件内容\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "           md_text = f.read()\n",
    "           html_text = markdown.markdown(md_text)\n",
    "           # 使用BeautifulSoup（html_text, 'html.parser'）\n",
    "           soup = BeautifulSoup(html_text, 'html.parser')\n",
    "           plain_text = soup.get_text()\n",
    "           # 使用正则表达式一次网址链接\n",
    "           text = re.sub(r'http\\S+', '', plain_text)\n",
    "           return text\n",
    "    \n",
    "    @classmethod\n",
    "    def read_text(cls, file_path:str):\n",
    "        \"\"\"\n",
    "        读取txt文件内容\n",
    "        param file_path: 文件路径\n",
    "        return： 文件内容\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Documents:\n",
    "    \"\"\"\n",
    "    文档类：用于读取已分好类的json格式文档\n",
    "    \"\"\"\n",
    "    def __init__(self, path:str = '') -> None:\n",
    "        self.path = path\n",
    "    \n",
    "    def get_content(self):\n",
    "        \"\"\"\n",
    "        读取 json 格式的文档内容\n",
    "        return json 文档的内容\n",
    "        \"\"\"\n",
    "        with open(self.path, 'r', encoding='utf-8') as f:\n",
    "            content = json.load(f)\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 ReadFiles类，指定文件目录路径data\n",
    "file_reader = ReadFiles('data')\n",
    "\n",
    "# 获取目录下所有支持的文件类型\n",
    "file_list = file_reader.get_files()\n",
    "print(\"支持的文件类型：\", file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chunks = file_reader.get_content(max_token_len=600, cover_content=150)\n",
    "print(\"分块后的文档内容：\", document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document_chunks[0])\n",
    "print(document_chunks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 词向量数据库与向量检索模块\n",
    "\n",
    "为了构建向量数据库，需要一下关键功能：\n",
    "\n",
    "1. **持久化存储（persist）：** 将数据库存储到本地，便于此次加载使用\n",
    "2. **加载数据库（load_vector）：** 从本地文件加载已经存储的向量和文档。\n",
    "3. **获取向量表示（get_vector）：** 将文本转化为向量表示并存储\n",
    "4. **检索（query）:** 根据用户的Query，检索数据库中的想啊滚文档片段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, document: List[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        初始化向量存储类，存储文档和对应的向量表示\n",
    "        param document：文档列表，默认为空\n",
    "        \"\"\"\n",
    "        if document is None:\n",
    "            documnt = []\n",
    "        self.document = document # 存储文档内容\n",
    "        self.vector = [] # 存储文档的向量表示\n",
    "\n",
    "    def get_vector(self, EmbeddingModel:BaseEmbeddings) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        使用传入的 Embedding 模型当文档向量化\n",
    "        param EmbeddingMode： 传入的用于生成向量的模型（需继承 BaseEmbedings 类）\n",
    "        return 返回文档对应的向量列表 \n",
    "        \"\"\"\n",
    "        # 遍历所有文档，获取每个文档的向量表示\n",
    "        self.vectors = [EmbeddingModel.get_embedding(doc) for doc in self.document]\n",
    "        return self.vectors\n",
    "    \n",
    "    def persist(self, path:str='storage'):\n",
    "        \"\"\"\n",
    "        将文本和对应的向量表示持久化本地目录中，以便后续加载使用\n",
    "        param path：存储路径，默认为：storage\n",
    "        \"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path) # 如果路径不存在，创建路径\n",
    "        # 保存向量为 numpy 文件\n",
    "        np.save(os.path.join(path, 'vectors.npy'), self.vectors)\n",
    "        # 将文档内容存储到文本文件中\n",
    "        with open(os.path.join(path, 'documents.txt'), 'w') as f:\n",
    "            for doc in self.document:\n",
    "                f.write(doc + '\\n')\n",
    "\n",
    "    def load_vector(self, path:str='storage'):\n",
    "        # 加载保存的向量数据\n",
    "        self.vectors = np.load(os.path.join(path, 'vectors.npy')).tolist()\n",
    "        # 加载文档内容\n",
    "        with open(os.path.join(path, 'documents.txt'), 'r') as f:\n",
    "            self.document = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    def get_similarity(self, vector1:List[float], vector2:List[float]) -> float:\n",
    "        \"\"\"\n",
    "        计算两个向量的预选相似度\n",
    "        param vector1: 第一个向量\n",
    "        param vector2: 第二个向量\n",
    "        return: 返回两个向量的余弦相似度 -1到1\n",
    "        \"\"\"\n",
    "        dot_product = np.dot(vector1, vector2)\n",
    "        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)\n",
    "        return dot_product / magnitude\n",
    "\n",
    "    def query(self, query:str, EmbeddingModel:BaseEmbeddings, k:int = 1)->List[str]:\n",
    "        \"\"\"\n",
    "        根据用户的查询文本，检索最相关的文档片段\n",
    "        param query: 用户查询的文本\n",
    "        param EmbeddingModel: 用于生成查询文本向量的模型\n",
    "        param k: 返回最相关的 k 个文档片段，默认为1\n",
    "        return: 返回最相关的 k 个文档片段\n",
    "        \"\"\"\n",
    "        # 将查询文本向量化\n",
    "        query_vector = EmbeddingModel.get_embedding(query)\n",
    "        # 计算查询向量与每个文档向量的相似度\n",
    "        similarities = [self.get_similarity(query_vector, vec) for vec in self.vectors]\n",
    "        # 获取最相关的 k 个文档片段\n",
    "        top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        # 返回对应的文档内容\n",
    "        return [self.document[idx] for idx in top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "documents = [\n",
    "    \"机器学习是人工智能的一个分支。\",\n",
    "    \"深度学习是一种特殊的机器学习方法\",\n",
    "    \"监督学习是一种训练模型的方法\",\n",
    "    \"强化学习是通过奖励和惩罚进行学习。\",\n",
    "    \"无监督学习不依赖标签数据\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建向量数据库\n",
    "vector_store = VectorStore(documents)\n",
    "\n",
    "# 使用 OpenAI Embedding 模型对文档进行向量化\n",
    "embedding_model = OpenAIEmbedding()\n",
    "\n",
    "# 获取文档向量并存储\n",
    "vector_store.get_vector(embedding_model)\n",
    "\n",
    "# 持久化存储到本地\n",
    "vector_store.persist('storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟用户查询\n",
    "query = \"什么是深度学习？\"\n",
    "result = vector_store.query(query, embedding_model)\n",
    "\n",
    "print('检索结果：', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_array = np.load('storage/vectors.npy')\n",
    "print('加载的向量：', loaded_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 大模型问答模块\n",
    "\n",
    "先创建基类`BaseModel`，然后再以 GPT4o 模型为例，展示如何使用大语言模型里完成这个任务\n",
    "\n",
    "### 5.1 大模型模块的基类\n",
    " - chat：负责处理用户的输入并生成回答\n",
    " - load_model：如果是使用本地模型，这个方法负责加载模型。如果使用API模型，可以不用实现这个方法。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    \"\"\"\n",
    "    基础模型类，作为所有模型的基类\n",
    "    包含一些通用的接口，如加载模型、生成回答等\n",
    "    \"\"\"\n",
    "    def __init__(self, path:str='') -> None:\n",
    "        self.path = path\n",
    "\n",
    "    def chat(self, prompt:str, history:List[dict], content:str) -> str:\n",
    "        \"\"\"\n",
    "        处理用户的输入并生成回答\n",
    "        param query: 用户输入的查询\n",
    "        return: 模型生成的回答\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load_model(self) -> None:\n",
    "        \"\"\"\n",
    "        加载模型\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 借助GPT4o模型进行对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT4oChat(BaseModel):\n",
    "    \"\"\"\n",
    "    GPT4o模型对话类\n",
    "    \"\"\"\n",
    "    def __init__(self, api_key:str=api_key\n",
    "                 , base_url:str=base_url) -> None:\n",
    "        super().__init__()\n",
    "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "\n",
    "    def chat(self, prompt:str, history:List=[], content:str='') -> str:\n",
    "        \"\"\"\n",
    "        处理用户的输入并生成回答\n",
    "        param prompt：用户的提问\n",
    "        param query: 用户输入的查询（可选）\n",
    "        param content：可参考的上下文信息（可选）\n",
    "        return：生成的回答\n",
    "        \"\"\"\n",
    "        # 构建包含问题和上下文的完整提示\n",
    "        full_prompt = PROMPT_TEMPLATE['GPT4o_PROMPT_TEMPLATE'].format(question=prompt, context=content)\n",
    "\n",
    "        # 调用 GPT-4o 模型进行推理\n",
    "        response = self.client.chat.completions.create(\n",
    "            model='deepseek-r1',\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": full_prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 返回模型生成的第一个答案\n",
    "        return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 提示模版\n",
    "为了方便维护和复用提示词，可以使用一个字典来保存不同模型的提示模版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = dict(\n",
    "    GPT4o_PROMPT_TEMPLATE = \"\"\"\n",
    "下面有一个或许与这个问题相关的参考段落，若你觉得参考段落能和问题相关，这现总结参考段落的内容。\n",
    "若你觉得参考段落和问题无关，这使用你自己的原始知识来回答用户的问题，并且送死使用中文来回答\n",
    "问题：{question}\n",
    "可参考的上下文\n",
    "···\n",
    "{context}\n",
    "···\n",
    "有用的回答：\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 完整流程演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载并切分文档\n",
    "docs = ReadFiles('./data').get_content(max_token_len=600, cover_content=150)\n",
    "vector = VectorStore(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用OpenAI Embedding 模型惊进行向量化\n",
    "embedding = OpenAIEmbedding()\n",
    "vector.get_vector(EmbeddingModel=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将向量和文本保存到本地\n",
    "vector.persist(path = 'storage')\n",
    "# 用户提出问题\n",
    "question = 'RAG技术的基本原理是什么？'\n",
    "# 在数据库中检索最相关的文档片段\n",
    "content = vector.query(question, EmbeddingModel=embedding, k=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 GPT40Chat 模型生成答案\n",
    "chat = GPT4oChat()\n",
    "print(chat.chat(question, [], content)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mini_rag(question:str, knowledge_base_path:str, k:int=1) -> str:\n",
    "    \"\"\"\n",
    "    运行一个简化版的RAG项目\n",
    "\n",
    "    param question: 用户提出的问题\n",
    "    param knowledge_base_path：知识库的路径，包含文档的文件夹路径\n",
    "    param api_key：OpenAI API密钥，用于调用GPT-4o模型\n",
    "    param k：返回与文艺最想概念的k个文档片段，默认为1\n",
    "    return：返回GPT-4o模型生成的回答\n",
    "    \"\"\"\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    # 1. 加载并切分文档\n",
    "    docs = ReadFiles(knowledge_base_path).get_content(max_token_len=600, cover_content=150)\n",
    "    vector = VectorStore(docs)\n",
    "    # 2. 使用OpenAI Embedding 模型惊进行向量化\n",
    "    embedding = OpenAIEmbedding()\n",
    "    vector.get_vector(EmbeddingModel=embedding)\n",
    "    # 3. 将向量和文本保存到本地（可选）\n",
    "    vector.persist(path = 'storage')\n",
    "    # 4. 在数据库中检索最相关的文档片段\n",
    "    content = vector.query(question, EmbeddingModel=embedding, k=k)[0]\n",
    "    # 5. 使用GPT-4o模型生成答案\n",
    "    chat = GPT4oChat(api_key=api_key)\n",
    "    answer = chat.chat(question, [], content)\n",
    "\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'RAG技术的基本原理是什么？'\n",
    "knowledge_base_path = './data'\n",
    "answer = run_mini_rag(question, knowledge_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
